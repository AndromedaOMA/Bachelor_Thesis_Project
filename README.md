<h1 align="center">Hi ðŸ‘‹, here we have my Bachelor Thesis Project/Speech Enhancement project</h1>
<h3 align="center">Deep learning focused on improving the clarity of human language regardless of the patient's environment!</h3>


## Table Of Content
* [About Project](#project)
* [Architecture Overview](#architecture)
* [Statistics and Results](#statistics)
* [Dataset](#dataset)
* [Samples](#samples)
* [Conclusion](#conclusion)
* [Getting Started](#getting-started)

--------------------------------------------------------------------------------
<h1 id="project" align="left">ðŸ¤– About Project</h1>

In other words, this project will adjust and adapt in real time the hearing depending on the environment with the emphasis on the quality of the conversation. There are already technologies that address this topic, but most of them focus on canceling background noise ("noise reduction"), thus leaving only human sounds/conversations to be heard, without any intervention on human voices. This project will focus strictly on finding a more efficient solution based on the provided model.

---

<h1 id="architecture" align="left">ðŸ§  Architecture Overview</h1>

The FSPEN (Full-band and Sub-band Path Extension Network) model offers us a solution that combines concepts such as GRU, together with Convolutional Neural Networks (CNN) all combined through the Encoder-Decoder architecture.

Also, this architecture is ultra-lightweight (34.7960K parameters) and allows for rapid training and testing on the chosen dataset, achieving scores of various metrics, similar to the scores generated by models that reach parameters in the millions!

For better clarity of the statistics, we provide you with plot graphs that will highlight the quality and efficiency of the FSPEN model in comparison to other models with the same objective (data are taken from [link]([url](https://research.samsung.com/blog/FSPEN-AN-ULTRA-LIGHTWEIGHT-NETWORK-FOR-REAL-TIME-SPEECH-ENAHNCMENT))). Figure 3.5.1 presents the PESQ and STOI metrics for the models:

<img src="https://github.com/user-attachments/assets/383e380b-feea-41a0-b9ed-cf00f404c980" alt="Model Diagram" width="500" align="right">

| Model            | Parameters |
|------------------|------------|
| RNNoise          | 0.06 M     |
| CCFNet+ (Lite)   | 160 K      |
| CCFNet+          | 620 K      |
| DeepFilterNet2   | 2.31 M     |
| DeepFilterNet    | 1.78 M     |
| PercepNet        | 8 M        |
| FSPEN            | 34.7960 K  |

The FSPEN architecture is made up of three major components that, combined, bring efficiency to the forefront when processing audio files, as highlighted in Figure 3.5.4. These are:

  1. *The encoding module*: this, in turn, is divided into a sub-band encoder and a full-band encoder, which will lead to a capture of global and local features.
  2. *Dual Path Amplifier with Inter-Frame Path Extension (DPE) module*: improves network shaping capability while maintaining/keeping complexity low.
  3. *Decoding module*: like the encoding module, it is divided, this time into a sub-band decoder and a full-band decoder, the two decoders obtain the complexity spectrum mask and the magnitude spectrum mask.

<h3>FullSubPathExtension(FSPEN) profile (what does the model use)</h3>
[INFO] Register count_convNd() for class 'torch.nn.modules.conv.Conv1d'.</br>
[INFO] Register count_normalization() for class 'torch.nn.modules.batchnorm.BatchNorm1d'.</br>
[INFO] Register zero_ops() for class 'torch.nn.modules.activation.ReLU'.</br>
[INFO] Register count_linear() for class 'torch.nn.modules.linear.Linear'.</br>
[INFO] Register zero_ops() for class 'torch.nn.modules.container.Sequential'.</br>
[INFO] Register count_gru() for class 'torch.nn.modules.rnn.GRU'.</br>
[INFO] Register count_normalization() for class 'torch.nn.modules.normalization.LayerNorm'.</br>
[INFO] Register count_convNd() for class 'torch.nn.modules.conv.ConvTranspose1d'.</br>
</br>
<h3>Original FullSubPathExtension(FSPEN) architecture</h3>
<img src="https://github.com/user-attachments/assets/9576cc8c-a42b-4bb6-83b9-f0a18baff4f4" alt="Model Diagram" width="600" align="center">
</br>
<h3>Original FSPEN Model</h3>
FLOPs: 152.7967M</br>
Params: 34.7960K</br>
</br>
<h3>Modified FullSubPathExtension(FSPEN) architecture</h3>
<img src="https://github.com/user-attachments/assets/5f4b9608-bcb2-4f59-8167-9be3374e2f51" alt="Modified Model Diagram" width="600" align="center">
</br>
<h3>Modiffied FSPEN Model [FrequencyAttention that includes a MultiheadAttention]</h3>
FLOPs: 154.8691M</br>
Params: 34.8930K</br>
</br>

**Result**: The new FSPEN model has very similar dimensions to the original, thus maintaining its flexibility and essential character as an ultra-light model. That said, the original model has 34.7960K parameters, while the modified model has 34.8930K parameters. That is, a difference of only 0.097K parameters.

| Original FSPEN parameters  | Modified FSPEN parameters |
|----------------------------|---------------------------|
| 34.7960K                   | 34.8930K                  |

* [Table Of Content](#table-of-content)

---

<h1 id="statistics" align="left">ðŸ“ˆ Statistics and Results</h1>

In the histograms in Figure 1, significant differences can be observed between the original and modified models. The maximum frequency limit of audio sequences per evaluation metric increases drastically, but only for a slightly higher score than the original FSPEN model. This indicates the efficiency that the attention module introduced at the FSPEN model brings at the local level.
</br>
For better highlighting, we can also consult the table below:
|                 | STOI gain      |             | PESQ gain     |             |
|-----------------|------------------|-------------|------------------|-------------|
|                 | Original FSPEN   | Modified FSPEN | Original FSPEN   | Modified FSPEN |
| Maximum limit index | -0.025           | -0.015      | 0.20-0.25        | 0.25        |
| Frequency of audio sequences | 108              | 119         | 138              | 145         |
</br>
<img src="https://github.com/user-attachments/assets/8b5ef5c5-213a-4573-adc9-9e9e6f7773b9" alt="histogramÄƒ-frecvenÈ›a_per_cÃ¢È™tig_stoi_pesq" width="600" align="center">
</br>
Figure 1
</br>
</br>
In Figure 2, we can see how the multitude of sequences, which do not fall within those upper limits, pull down the entire PESQ/STOI gain. The median of each box indicates the middle value of the data distribution. In both plots, we can see how the median of the plots associated with the modified model indicates slightly lower values â€‹â€‹than those associated with the original model. We can also see the points/sequences outside the quadrants. These represent the extremes that pull down/up. We have a multitude of extremes that are not taken into account by the plots that indicate the increased variety of the samples.
</br>
<img src="https://github.com/user-attachments/assets/95fa7c86-14e7-49a0-906e-0f4269d69c6b" alt="distribuÈ›iile_cÃ¢È™tigului_pesq_stoi" width="600" align="center">
</br>
Figure 2
</br>
</br>
Figure 3 is a representation of where each audio sequence falls on the PESQ/STOI gain distribution. It is clearly seen that the distribution tends to fall in the southeast quadrant, that is, the quadrant with low STOI gain values â€‹â€‹and high PESQ gain values. The conclusion is that the PESQ gain is the majority.
</br>
<img src="https://github.com/user-attachments/assets/73439a3e-ecfc-48f5-9d88-56e5ec5b675d" alt="distribuÈ›iile_cÃ¢È™tigului_pesq_È™i_stoi" width="600" align="center">
</br>
Figure 3
</br>
</br>
Figure 4 gives us a much more detailed picture of the advantages and disadvantages of introducing the attention module at the level of comparison between noisy and improved sequences. As will be seen, the median lines of each PESQ/STOI metric within the improved sound samples of the modified model tend to be slightly lower than those of the original model. This clearly indicates that the attention module obtains the desired results only for a narrower range of audio sequences, while for the rest it actually worsens them (more clearly in figure 3.6.2). However, the differences are relatively small, as a result we can also conclude that regardless of which of the models, they improve the performance in terms of speech quality and perception (PESQ), but the results in terms of speech intelligibility assessment (STOI) are somewhat degraded compared to PESQ.
</br>
<img src="https://github.com/user-attachments/assets/7a9d3cfb-d32e-41fd-8f0f-17acba58bc5f" alt="comparaÈ›ii_Ã®ntre_metricile_stoi_pesq_a_secvenÈ›elor_cu_zgomot_È™i_Ã®mbunÄƒtÄƒÈ›ite" width="600" align="center">
</br>
Figure 4
</br>
</br>
Figure 5 adds value to our conclusions. Below we see the distribution of noisy and enhanced audio samples on the PESQ/STOI metrics.
</br>
  a. **PESQ Metric**: The main diagonal of each plot delimits the portion associated with enhanced sequences (PESQ Enhanced) versus that associated with noisy sequences (PESQ Noisy) which targets speech clarity and perception. Both plots indicate similar values, for example both plots have most of the points/samples placed above the main diagonal/axis, towards the PESQ Enhanced portion, thus indicating an increase in speech clarity. The original FSPEN model (left) expands and scatters audio sequences over a larger area. This suggests that the model may be less consistent with higher quality noisy samples, sometimes over-amplifying or under-amplifying them. While the modified FSPEN model (right) concentrates all of the points in a cluster closer to low to medium values â€‹â€‹of the PESQ metric. The attention module seems to improve the model and make it more consistent, especially for noisy samples with PESQ between 1.0 and 2.5. </br> </br> 

**Conclusions**: The modified model - </br>
    i. It shows better consistency in improved PESQ scores, especially at medium and low noise PESQ. </br>
    ii. It can avoid some of the over- or under-amplification issues seen in the original model. </br>
    iii. It seems to generalize better over a wider range of input PESQ levels. </br>
</br>
</br>
  b. **STOI Metric**: The main diagonal of each plot delimits the portion associated with the enhanced sequences (STOI Enhanced) compared to that associated with the noisy sequences (STOI Noisy) which aims to evaluate the speech intelligibility. Both plots indicate similar values, for example both plots have the majority of points/samples placed on/above the main diagonal/axis, which highlights that both models contribute to speech intelligibility. This time the original model (left) seems to provide more optimal results at the level of the STOI metric, which is observed at the level of the tendency of the audio sequences to be placed above the oblique axis, compared to the modified model. However, the attention module manages to improve the total scores by 10% compared to the initial case. </br> </br>

**Conclusions**: The modified model - </br>
    i. It seems to stabilize the results this time too by strongly clustering the samples and avoiding over- or under-amplification problems observed in the original model.
</br>
</br>
<img src="https://github.com/user-attachments/assets/dc0bff3b-9f28-47c8-93cb-8ca765f1b641" alt="comparaÈ›ii_Ã®ntre_metricile_pesq_stoi_a_secvenÈ›elor_cu_zgomot_È™i_Ã®mbunÄƒtÄƒÈ›ite" width="600" align="center">

</br>

* [Table Of Content](#table-of-content)

---

<h1 id="dataset" align="left">ðŸ“„ Dataset</h1>

In this case, we will approach the labeled dataset type. More specifically, we will work with the **VoiceBank + DEMAND** dataset that we can find [here](https://huggingface.co/datasets/JacobLinCool/VoiceBank-DEMAND-16k). This dataset will provide us with two types of audio files, each with a sampling rate of 16kHz:
  1. Audio files containing clear human conversations/voices ("clear speech" in English).
  2. Audio files containing unclear/noisy human conversations/voices ("noisy speech" in English) of background sounds that simulate real situations such as the noises we hear in everyday life in environments such as traffic, closed/open rooms, etc.

The training and testing of the model will be done through two partitions:
  1. The training partition containing a number of 11.6K data.
  2. The testing partition containing a number of 824 data.

---

<h1 id="samples" align="left">ðŸ”Š Samples</h1>

Dive into the sound quality of our models. We've prepared samples to showcase the difference!


| Sample Set | FPEN Noisy (Original)                                                 | FPEN Clean (Ground Truth) | FPEN Enhanced (Output) | FSPEN Noisy (Original)                                                | FSPEN Clean (Ground Truth) | FSPEN Enhanced (Output) |
| :--------- | :-------------------------------------------------------------------- | :------------------------ | :--------------------- | :-------------------------------------------------------------------- | :------------------------- | :---------------------- |
| **Set 1** | https://soundcloud.com/marius-alexandru-olaru/4sec_no_mha_noisy_0_7 | [Coming Soon!]            | [Coming Soon!]         | https://soundcloud.com/marius-alexandru-olaru/4sec_no_mha_noisy_0_7 | [Coming Soon!]             | [Coming Soon!]          |
| **Set 2** | [Coming Soon!]                                                        | [Coming Soon!]            | [Coming Soon!]         | [Coming Soon!]                                                        | [Coming Soon!]             | [Coming Soon!]          |
| **Set 3** | [Coming Soon!]                                                        | [Coming Soon!]            | [Coming Soon!]         | [Coming Soon!]                                                        | [Coming Soon!]             | [Coming Soon!]          |


Original FPEN model:
1. [noisy_sample 1](https://soundcloud.com/marius-alexandru-olaru/4sec_no_mha_noisy_0_7)
2. [clean_sample_1]()
3. [enhanced_samnple_1]()

Modified FSPEN model:
1. [noisy_sample 1](https://soundcloud.com/marius-alexandru-olaru/4sec_no_mha_noisy_0_7)
2. [clean_sample_1]()
3. [enhanced_samnple_1]()
   
---

<h1 id="conclusion" align="left">ðŸ”š Conclusion</h1>

So far, I have managed to add a "MultiHeadAttention" module to the architecture of the original model. I have trained, on several occasions, the original model (without modifications) and the modified one. Thus, after careful analysis of the plots made based on the PESQ and STOI evaluation metrics, I have come to the conclusion that the performance is slightly changed. Therefore, the modified FSPEN model (which contains the attention module) manages to avoid some of the over- or under-amplification/estimation problems observed in the original model. This is highlighted by plots that show a more pronounced clustering of the analyzed audio sequences and classified based on the PESQ/STOI evaluation metrics. We can further improve the performance by adding more performant and efficient concepts to the model.


---

<h1 id="getting-started" align="left">ðŸš€ Getting Started</h1>

1. Clone the repository:
``` git clone git@github.com:AndromedaOMA/Bachelor_Thesis_Project.git ```
2. Have fun!

---

> ðŸ“ **Note**:  
> By completing this bachelor's thesis, I am completing my bachelor's studies at the Faculty of Computer Sciences in IaÈ™i.<br/>
> The model architecture is taken from [here](https://research.samsung.com/blog/FSPEN-AN-ULTRA-LIGHTWEIGHT-NETWORK-FOR-REAL-TIME-SPEECH-ENAHNCMENT).

* [Table Of Content](#table-of-content)
