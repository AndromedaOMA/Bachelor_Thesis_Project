<h1 align="center">Hi 👋, here we have my Bachelor Thesis Project/Speech Enhancement project</h1>
<h3 align="center">Deep learning focused on improving the clarity of human language regardless of the patient's environment!</h3>


## Table Of Content
* [About Project](#project)
* [Architecture Overview](#architecture)
* [Dataset](#dataset)
* [Getting Started](#getting-started)

--------------------------------------------------------------------------------
<h1 id="project" align="left">🤖 About Project</h1>

In other words, this project will adjust and adapt in real time the hearing depending on the environment with the emphasis on the quality of the conversation. There are already technologies that address this topic, but most of them focus on canceling background noise ("noise reduction"), thus leaving only human sounds/conversations to be heard, without any intervention on human voices. This project will focus strictly on finding a more efficient solution based on the provided model.

---

<h1 id="architecture" align="left">🧠 Architecture Overview</h1>

The FSPEN (Full-band and Sub-band Path Extension Network) model offers us a solution that combines concepts such as GRU, together with Convolutional Neural Networks (CNN) all combined through the Encoder-Decoder architecture.

Also, this architecture is ultra-lightweight (34.7960K parameters) and allows for rapid training and testing on the chosen dataset, achieving scores of various metrics, similar to the scores generated by models that reach parameters in the millions!

For better clarity of the statistics, we provide you with plot graphs that will highlight the quality and efficiency of the FSPEN model in comparison to other models with the same objective (data are taken from [link]([url](https://research.samsung.com/blog/FSPEN-AN-ULTRA-LIGHTWEIGHT-NETWORK-FOR-REAL-TIME-SPEECH-ENAHNCMENT))). Figure 3.5.1 presents the PESQ and STOI metrics for the models:

<img src="https://github.com/user-attachments/assets/383e380b-feea-41a0-b9ed-cf00f404c980" alt="Model Diagram" width="500" align="right">

| Model            | Parameters |
|------------------|------------|
| RNNoise          | 0.06 M     |
| CCFNet+ (Lite)   | 160 K      |
| CCFNet+          | 620 K      |
| DeepFilterNet2   | 2.31 M     |
| DeepFilterNet    | 1.78 M     |
| PercepNet        | 8 M        |
| FSPEN            | 34.7960 K  |

The FSPEN architecture is made up of three major components that, combined, bring efficiency to the forefront when processing audio files, as highlighted in Figure 3.5.4. These are:

  1. *The encoding module*: this, in turn, is divided into a sub-band encoder and a full-band encoder, which will lead to a capture of global and local features.
  2. *Dual Path Amplifier with Inter-Frame Path Extension (DPE) module*: improves network shaping capability while maintaining/keeping complexity low.
  3. *Decoding module*: like the encoding module, it is divided, this time into a sub-band decoder and a full-band decoder, the two decoders obtain the complexity spectrum mask and the magnitude spectrum mask.
     
<img src="https://github.com/user-attachments/assets/9576cc8c-a42b-4bb6-83b9-f0a18baff4f4" alt="Model Diagram" width="600" align="center">
</br>
<h3>FullSubPathExtension(FSPEN) profile</h3>
[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv1d'>.</br>
[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm1d'>.</br>
[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.</br>
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.</br>
[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.</br>
[INFO] Register count_gru() for <class 'torch.nn.modules.rnn.GRU'>.</br>
[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.</br>
[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.ConvTranspose1d'>.</br>
</br>
<h3>Original FSPEN Model</h3>
FLOPs: 152.7967M</br>
Params: 34.7960K</br>
</br>
<h3>Modiffied FSPEN Model [FrequencyAttention that includes a MultiheadAttention]</h3>
FLOPs: 154.8691M</br>
Params: 34.8930K</br>

---

<h1 id="dataset" align="left">📄 Dataset</h1>

In this case, we will approach the labeled dataset type. More specifically, we will work with the **VoiceBank + DEMAND** dataset that we can find [here](https://huggingface.co/datasets/JacobLinCool/VoiceBank-DEMAND-16k). This dataset will provide us with two types of audio files, each with a sampling rate of 16kHz:
  1. Audio files containing clear human conversations/voices ("clear speech" in English).
  2. Audio files containing unclear/noisy human conversations/voices ("noisy speech" in English) of background sounds that simulate real situations such as the noises we hear in everyday life in environments such as traffic, closed/open rooms, etc.

The training and testing of the model will be done through two partitions:
  1. The training partition containing a number of 11.6K data.
  2. The testing partition containing a number of 824 data.

---

<h1 id="getting-started" align="left">🚀 Getting Started</h1>

1. Clone the repository:
``` git clone git@github.com:AndromedaOMA/Bachelor_Thesis_Project.git ```
2. Have fun!

---

> 📝 **Note**:  
> By completing this bachelor's thesis, I am completing my bachelor's studies at the Faculty of Computer Sciences in Iași.<br/>
> The model architecture is taken from [here](https://research.samsung.com/blog/FSPEN-AN-ULTRA-LIGHTWEIGHT-NETWORK-FOR-REAL-TIME-SPEECH-ENAHNCMENT).

* [Table Of Content](#table-of-content)
